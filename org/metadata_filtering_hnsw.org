
#+TITLE: Metadata Filtering for HNSW Dense Vectors - Detailed Design Specification
#+AUTHOR: Nithin Mani
#+DATE: 2025-01-01

* Overview

This document describes the design for implementing metadata-based filtering capabilities in the CosData vector database, specifically for HNSW dense vectors. The core innovation lies in extending vector dimensions during quantization to encode metadata information, enabling efficient filtering while maintaining the effectiveness of similarity search.

* Index Structure

** Base Vector Representation

All vectors in the system, whether used for pure similarity search or metadata filtering, must maintain the same dimensionality for HNSW graph construction and search. To achieve this, the base vectors (e.g., 768d) are extended with phantom dimensions for each metadata field, but these dimensions are not assigned the high-weight values (1024) used in metadata filtering. These phantom dimensions ensure dimensional consistency across all vector copies while not affecting similarity calculations for non-filtered searches.

** Metadata Dimension Encoding

The encoding of metadata values into vector dimensions follows a carefully designed scheme that balances accuracy, efficiency, and storage requirements. Each metadata field requires a specific number of additional dimensions based on its cardinality (number of possible values).

*** Dimension Allocation

For each metadata field, the number of required dimensions is calculated by rounding up the field's cardinality to the nearest power of 2. This power-of-2 allocation ensures efficient binary encoding of values. For example:

- A field representing months (12 possible values) requires 4 dimensions (equivalent to 2^4 = 16 binary bits)
- A field for days of week (7 values) requires 3 dimensions (equivalent to 2^3 binary bits)
- A binary field (2 values) requires just 1 dimension (equivalent to 2^1 binary bits)
- A field with 100 possible values requires 7 dimensions (equivalent to 2^7 binary bits)

*** Metadata schema
At the time of creating a collection, the user needs to specify the "metadata schema" which will contain information such as,

- all the metadata fields that need filtering
- set of unique values for each metadata field (which will give us the cardinality). We will implicitly sort these unique values e.g. lexicographically so that we can (implicitly) map them to numeric values that can be represented using binary bits.
- which AND/OR queries need to be supported so that we can create replica vectors only for those combinations. Only the support for AND/OR queries needs to be explicitly specified, filtering by individual fields must be implicitly supported.

The metadata schema will be stored in lmdb.

Refer to the "Create collection" [[api-doc.org][API doc]], where an example of "metadata_schema" in request body is included.

When the client/user sends an insert request with a metadata value that's not defined in the schema, then an error response will be returned.

*** Updating dimensions / cardinality of fields
The unique set of values supported for a metadata will be declared in the metadata schema specified in the ~create collection~ API call. However, it may happen that the user wants to add a new value to an existing field.

This will cause the cardinality to change but because we're roundingup the field's cardinality to the nearest power of two, there may be some room for the new value(s) without having to change the dimensions allocated for the field. If there's no room, then we'll have to reindex the collection i.e. create all logical indexes for different metadata filtering scenario (Ref: [[*Metadata Filtering Index]] section below to know more about logical indexes).

In case a new metadata field itself needs to be added, then the number of phantom dimensions itself changes, hence the collection will have to be reindexed.

*** Quantization Values

These metadata dimensions use a fundamentally different quantization scheme compared to the original vector dimensions:

- Original vector dimensions typically use small quantization values (0-255 for uint8)
- Metadata dimensions use much larger values (1024) during quantization
- This large value creates a strong signal during dot product calculations
- The high weight ensures metadata filtering takes precedence over similarity in the original dimensions

Example: Consider a vector with 768 original dimensions extended with metadata fields for:
- Month (4 dimensions)
- Day of week (3 dimensions)
- Binary category (1 dimension)

The final vector would have 776 dimensions (768 + 4 + 3 + 1), with the metadata dimensions using the 1024 quantization value to encode their respective field values.

The order in which the phantom dimensions will be added to the original vector will be implicit. Example: We may sort the metadata fields specified in the ~metadata_schema~ in lexicographically. This user/client doesn't need to know about this. If the metadata fields change, then the collection needs to be reindexed anyway.

This encoding scheme ensures that:
1. Each possible metadata value has a unique binary representation
2. The high quantization values make metadata filters definitive in search results
3. The power-of-2 dimension allocation prevents value overlap
4. Storage overhead is optimized by using the minimum required dimensions for each field's cardinality

** Metadata Filtering Index

The unified index structure contains multiple copies of each vector, where each copy is optimized for different metadata filtering scenarios. While these appear logically separate, they are physically stored within a single HNSW graph structure sharing a common root node. The key aspects are:

1. Base Vector Copy: Contains phantom dimensions but no high-weight values
2. Combined Field Copies: For AND query support, vectors are copied with high-weight values for multiple field combinations. The field combinations that need to be supported will be specified in the ~metadata_schema~ at the time of collection creation
3. All fields combined copy: For OR query support, a single copy with high-weight values for all the fields is sufficient. This way, we avoid create copies for individual fields. If the ~metadata_schema~ includes ~AND~ query support for all fields, then this copy will be created anyway (point 2 above).

* Vector Extension and Indexing Process

The indexing process requires multiple passes for each vector:

1. First Pass: Index the base vector with phantom dimensions (no 1024 weights)
2. Subsequent Passes: For each required metadata field combination (~AND~ query support):
  - Create a copy of the vector
  - Set appropriate high-weight values (1024) in the phantom dimensions for all relevant fields
  - Index this copy
3. Additional Pass (Optional): For all fields combination (~OR~ query support):
  - Create a copy with high-weight values for all fields
  - Index this combination copy
  - This step is optional because if the ~metadata_schema~ requires ~AND~ field support for all the fields, then this copy would have already been created in step 2.

All these passes contribute to building the unified HNSW graph structure. The phantom dimensions ensure consistent vector dimensionality across all copies, while the high-weight values enable effective filtering during queries.

* Query Vector Encoding

The effectiveness of metadata filtering relies on a carefully designed query vector encoding scheme that uses +1/-1 values to ensure precise matching. This encoding scheme is fundamental to supporting both equality and inequality filters.

** Equality Filter Encoding

When searching for vectors with a specific metadata value, the system employs a binary encoding strategy across the dimensions allocated for that field. For example, when filtering for value 1 in a field, the query vector would have:
- A positive value (+1) in the position corresponding to bit 0
- A negative value (-1) in the position corresponding to bit 1
- Similar negative values in all other bit positions for that field

This encoding ensures accurate discrimination between different values. For instance, when searching for value 1, a vector with value 3 (binary 11) will not match because the negative query value at position 1 will create a repelling force in the dot product calculation, effectively eliminating false matches.

** Inequality Filter Encoding

For inequality filters (field != x), the system inverts the encoding used in equality filters. Taking the same example of filtering for "not equal to 1":
- The positive and negative values from the equality encoding are inverted
- Position 0 becomes -1
- Position 1 becomes +1
- Other positions retain appropriate values to maintain filtering accuracy

During dot product calculations, these inverted values create attractive forces for all values except the one being excluded, effectively implementing the inequality constraint.

The high-weight values (1024) used in the indexed vectors, combined with the +1/-1 encoding in query vectors, create substantial differences in dot product results between matching and non-matching vectors. This ensures reliable filtering even in the presence of approximate nearest neighbor search.

* Query Processing

** Pure Similarity Search

When no metadata filtering is needed, queries use vectors with phantom dimensions (no high weights), effectively matching the base vector copies in the index.

** Metadata Filtering Queries

For metadata-filtered searches, the query vector is constructed with appropriate +1/-1 values in the phantom dimensions:
- +1 for matching the desired value's position
- -1 for other positions to prevent false matches

The system automatically routes the query to the appropriate vector copies based on the filtering criteria:
- Single field filters use the all-fields copy
- OR conditions require multiple searches using all-fields copy, with results merged via map-reduce
- AND conditions use the pre-computed combination copies


* Computing Cosine Similarity

This section describes an optimized approach for computing cosine similarity by first checking alignment in a subset of dimensions (Metadata) before computing full similarity.

The approach works by splitting the vector into two parts:

    - Metadata (M): A subset of dimensions (e.g., last 10 dimensions).
    - Values (V): The remaining dimensions (e.g., first 90 dimensions).

** Step 1: Compute Cosine Similarity for Metadata (M)
First, compute cosine similarity using only the metadata dimensions.

NOTE: the pseudocode is meant only for showing the logic, need to use appropriate quantization & SIMD functions for the Value computations, the Metadata computations can be done with scalar u32.

#+BEGIN_SRC rust
fn compute_metadata_similarity(A: &[f64], B: &[f64], metadata_indices: &[usize]) -> f64 {
    let dot_product: f64 = metadata_indices.iter().map(|&i| A[i] * B[i]).sum();
    let norm_A = metadata_indices.iter().map(|&i| A[i] * A[i]).sum::<f64>().sqrt();
    let norm_B = metadata_indices.iter().map(|&i| B[i] * B[i]).sum::<f64>().sqrt();
    dot_product / (norm_A * norm_B)
}
#+END_SRC

** Step 2: Compute Full Cosine Similarity if Needed
Only if the metadata similarity is ~1 (consider a small epsilon for floating point rounding), compute full similarity.

** Compute Dot Product for Values (V)
#+BEGIN_SRC rust
fn compute_dot_product(A: &[f64], B: &[f64], value_indices: &[usize]) -> f64 {
    value_indices.iter().map(|&i| A[i] * B[i]).sum()
}
#+END_SRC

** Compute Norms for Values (V)
#+BEGIN_SRC rust
fn compute_norm(A: &[f64], indices: &[usize]) -> f64 {
    indices.iter().map(|&i| A[i] * A[i]).sum::<f64>().sqrt()
}
#+END_SRC

** Compute Total Cosine Similarity
#+BEGIN_SRC rust
fn compute_full_similarity(A: &[f64], B: &[f64], metadata_indices: &[usize], value_indices: &[usize]) -> f64 {
    let S_M = compute_dot_product(A, B, metadata_indices);
    let S_V = compute_dot_product(A, B, value_indices);
    let norm_A = (compute_norm(A, metadata_indices).powi(2) + compute_norm(A, value_indices).powi(2)).sqrt();
    let norm_B = (compute_norm(B, metadata_indices).powi(2) + compute_norm(B, value_indices).powi(2)).sqrt();
    (S_M + S_V) / (norm_A * norm_B)
}
#+END_SRC

* Performance Implications

The unified index structure with phantom dimensions has several performance characteristics:

Storage Impact:
- Linear increase with number of metadata fields
- Additional increase for field combinations (AND support)
- All vectors maintain consistent dimensionality due to phantom dimensions

Memory Usage:
- Efficient memory utilization through unified index structure
- Overhead from phantom dimensions in all vector copies
- Memory requirements scale with number of supported metadata combinations

CPU Requirements:
- Multiple indexing passes for each vector
- Increased dimension count affects similarity calculations
- Query routing overhead based on filtering criteria

* Implementation Phases

1. Create spec for the ~metadata_schema~ field in ~Create collection~ API request body
2. Implement phantom dimension extension for base vectors
3. Develop indexing pipeline for multiple passes
4. Create unified index structure
5. Implement metadata-aware query processing
6. Add support for field combinations
7. Optimize performance and resource usage

* Future Considerations

- Optimize phantom dimension handling
- Smart selection of field combinations based on query patterns
- Compression techniques for redundant vector copies
- Dynamic generation of field combinations

* High level tasks

- [X] Create a spec for metadata schema
- [X] Implement core functionality for dimension allocation
  + Given the metadata schema find out how many phantom dimensions are required for metadata filtering
  + Create a vec of those many dimensions
  + Create phantom dimensions vec copy with,
    1. no high weight values (for base copy)
    2. high weight values for supported field combinations (for AND query support)
    3. high weight values for all field combination (for OR query support)
       + optional as it may be covered in (2)
- [-] Implement metadata-aware insertion
  + [X] Modify the create collection API endpoint to accept metadata_schema in above format
  + [ ] Include metadata fields in all "insertion" code paths
    + Validate metadata fields based on the metadata_schema for the collection
  + [ ] Implement the indexing pipeline for multiple phases
  + [ ] Modify all "insertion" API endpoints to accept metadata fields
    + Create collection
    + Create dense index
    + Create vector
    + Update vector
    + upsert
- [X] Implement query vector encoding core functionality as per the doc
  + Given the metadata filters and metadata_schema for the collection, extend the query vectors with +1/-1 values in the phantom dimensions.
    + We can start with individual fields and then add support for field combinations in a later phase.
- [ ] Implement metadata-aware query processing
  - Integrate query vector encoding core functionality
    - Modify ~VectorANN~ and ~BatchVectorANN~ structs to include metadata filters
    - Validation of metadata filters
  - Modify the "search" API endpoints to add metadata filters
    - search
    - batch_search
- [ ] New APIs for updating the metadata_schema [not sure about the priority]
